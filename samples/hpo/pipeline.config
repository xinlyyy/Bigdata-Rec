train_config {
  optimizer_config {
    use_moving_average: false
    adam_optimizer {
      learning_rate {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.001
          decay_steps: 1000
          decay_factor: 0.1
          min_learning_rate: 1e-7
        }
      }
    }
  }
  sync_replicas: true
  save_summary_steps: 20
  log_step_count_steps: 20
}
